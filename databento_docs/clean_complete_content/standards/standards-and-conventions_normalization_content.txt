# Normalization | Databento standards & conventions
Category: Standards
Source: standards-and-conventions_normalization.html
================================================================================

Quickstart
Set up Databento
Choose a service
Build your first application
New user guides
Examples and tutorials
Equities
Equities: Introduction
Top pre-market movers
Find average spread for a symbol
Futures
Futures: Introduction
Volume, open interest, and settlement prices
Futures trading hours
Options
Equity options: Introduction
Options on futures: Introduction
All options with a given underlying
Join options with underlying prices
US equity options volume by venue
Resample US equity options NBBO
Estimate implied volatility
Get symbols for 0DTE options
Live data
Handle multiple record types
Stream live data to a file
Estimate Databento feed latency
Calculate TICK and TRIN indicators
Subscribe to MBO snapshot
Compare on-exchange and off-exchange trade volume
Historical data
Request a large number of symbols
Programmatic batch downloads
Best bid, best offer, and midprice
Constructing OHLCV bars from the Trades schema
Join schemas on instrument ID
Plot a candlestick chart
Calculate VWAP and RSI
End-of-day pricing and portfolio valuation
Benchmark portfolio performance
Market halts, volatility interrupts, and price bands
Symbology
Continuous contracts
Parent symbology
Symbology mapping for live data
Dataset symbols
Instrument definitions
Finding liquid instruments
Handling tick sizes
Order book
Types of order book events
State management of resting orders
Limit order book construction
Microprice, book imbalance, and book pressure
Queue position of an order
Algorithmic trading
A high-frequency liquidity-taking strategy
Build prediction models with machine learning
Execution slippage and markouts
Matching engine latencies
Using messaging rates as a proxy for implied volatility
Mean reversion and portfolio optimization
Pairs trading based on cointegration
Build a real-time stock screener
Corporate actions
Dividends
New listings
Splits and reverse splits
Mergers and demergers
Adjustment factors
Applying adjustment factors
Handling multiple stock selections
Security master
Enrich instrument definitions
Listings and delistings
Market capitalization change
Core concepts
Schemas and data formats
What's a schema?
Market by order (MBO)
Market by price (MBP-10)
Market by price (MBP-1)
BBO on trade (TBBO)
BBO on interval (BBO)
Trades
Aggregate bars (OHLCV)
Instrument definitions
Imbalance
Statistics
Status
Corporate actions
Adjustment factors
Security master
Standards and conventions
Common fields, enums and types
Normalization
Symbology
Databento Binary Encoding
Zstandard (zstd)
MBO snapshots
Reference data enums
Architecture
Databento architecture
Timestamping
Locations and network connectivity
Dedicated connectivity
Databento NTP service
Performance optimization
Venues and datasets
CME Globex MDP 3.0
Cboe BYX Depth
Cboe BYZ Depth
Cboe EDGA Depth
Cboe EDGX Depth
Databento US Equities Basic
Databento US Equities Mini
Databento US Equities Summary
European Energy Exchange
Eurex Exchange
ICE Endex iMpact
ICE Europe Commodities iMpact
ICE Europe Financials iMpact
ICE Futures US iMpact
IEX TOPS
MEMX Memoir
MIAX Depth of Market
Nasdaq Basic with NLS Plus
Nasdaq TotalView-ITCH
NYSE American Integrated
NYSE Arca Integrated
NYSE Texas Integrated
NYSE National Trades and BBO
NYSE Integrated
OPRA Pillar
Corporate actions
Adjustment factors
Security master
API Reference
Historical API
Basics
Overview
Authentication
Schemas and conventions
Datasets
Symbology
Encodings
Compression
Dates and times
Errors
Rate limits
Size limits
Metered pricing
Versioning
Client
Historical
Metadata....list_publishers....list_datasets....list_schemas....list_fields....list_unit_prices....get_dataset_condition....get_dataset_range....get_record_count....get_billable_size....get_cost
Time series....get_range....get_range_async
Symbology....resolve
Batch downloads....submit_job....list_jobs....list_files....download....download_async
Helpers
DBNStore
....from_bytes....from_file....reader....replay....request_full_definitions....request_symbology....to_csv....to_df....to_file....to_json....to_ndarray....to_parquet....__iter__....insert_symbology_json
map_symbols_csv
map_symbols_json
Live API
Basics
Overview
Authentication
Sessions
Schemas and conventions
Datasets
Symbology
Dates and times
Intraday replay
Snapshot
System messages
Errors
Connection limits
Metered pricing
Error detection
Versioning
Recovering after a disconnection
Maintenance schedule
Client
Live
....add_callback....add_stream....add_reconnect_callback....block_for_close....start....stop....subscribe....terminate....wait_for_close....__aiter__....__iter__
Reference API
Basics
Overview
Authentication
Symbology
Dates and times
Errors
Rate limits
Client
Reference
Corporate actions....get_range
Adjustment factors....get_range
Security master....get_last....get_range
Resources
FAQs
Client libraries vs. APIs
Streaming vs. batch download
Usage-based pricing and credits
Instruments and products
Venues and publishers
MBP-1 vs. TBBO vs. BBO schemas
Portal
Data catalog
Batch download
Data usage
API keys
Download center
Team
Billing
Plans and live data
Release notes
0.38.1 - 2025-06-17
0.38.0 - 2025-06-10
0.37.1 - 2025-06-03
0.37.0 - 2025-06-03
0.36.0 - 2025-05-27
0.35.1 - 2025-05-20
0.35.0 - 2025-05-13
0.34.2 - 2025-05-06
0.34.1 - 2025-04-29
0.34.0 - 2025-04-22
0.33.0 - 2025-04-15
0.32.1 - 2025-04-07
0.32.0 - 2025-04-02
0.31.0 - 2025-03-18
0.30.0 - 2025-02-11
0.29.0 - 2025-02-04
0.28.0 - 2025-01-21
0.27.0 - 2025-01-07
0.26.0 - 2024-12-17
0.25.0 - 2024-11-12
0.24.0 - 2024-10-22
0.23.0 - 2024-09-25
0.22.0 - 2024-08-27
0.21.0 - 2024-07-30
0.20.1 - 2024-07-16
0.20.0 - 2024-07-09
0.19.1 - 2024-06-25
0.19.0 - 2024-06-04
0.18.1 - 2024-05-22
0.18.0 - 2024-05-14
0.17.1 - 2024-04-08
0.17.0 - 2024-04-01
0.16.0 - 2024-03-01
0.15.0 - 2024-01-16
0.14.1 - 2023-12-18
0.14.0 - 2023-11-23
0.13.1 - 2023-10-23
0.13.0 - 2023-09-21
0.12.0 - 2023-08-24
0.11.0 - 2023-08-10
0.10.0 - 2023-07-20
0.9.1 - 2023-07-11
0.9.0 - 2023-06-13
0.8.0 - 2023-05-16
0.7.0 - 2023-04-28
0.6.1 - 2023-03-28
0.6.0 - 2023-03-24
0.5.0 - 2023-03-13
0.4.0 - 2023-03-02
0.3.0 - 2023-01-06
0.2.0 - 2022-12-01
0.1.0 - 2022-11-07
Python
0.57.1 - 2025-06-17
0.57.0 - 2025-06-10
0.56.0 - 2025-06-03
0.55.1 - 2025-06-02
0.55.0 - 2025-05-29
0.54.0 - 2025-05-13
0.53.0 - 2025-04-29
0.52.0 - 2025-04-15
0.51.0 - 2025-04-08
0.50.0 - 2025-03-18
0.49.0 - 2025-03-04
0.48.0 - 2025-01-21
0.47.0 - 2024-12-17
0.46.0 - 2024-12-10
0.45.0 - 2024-11-12
0.44.1 - 2024-10-29
0.44.0 - 2024-10-22
0.43.1 - 2024-10-15
0.43.0 - 2024-10-09
0.42.0 - 2024-09-23
0.41.0 - 2024-09-03
0.40.0 - 2024-08-27
0.39.3 - 2024-08-20
0.39.2 - 2024-08-13
0.39.1 - 2024-08-13
0.39.0 - 2024-07-30
0.38.0 - 2024-07-23
0.37.0 - 2024-07-09
0.36.3 - 2024-07-02
0.36.2 - 2024-06-25
0.36.1 - 2024-06-18
0.36.0 - 2024-06-11
0.35.0 - 2024-06-04
0.34.1 - 2024-05-21
0.34.0 - 2024-05-14
0.33.0 - 2024-04-16
0.32.0 - 2024-04-04
0.31.1 - 2024-03-20
0.31.0 - 2024-03-05
0.30.0 - 2024-02-22
0.29.0 - 2024-02-13
0.28.0 - 2024-02-01
0.27.0 - 2024-01-23
0.26.0 - 2024-01-16
0.25.0 - 2024-01-09
0.24.1 - 2023-12-15
0.24.0 - 2023-11-23
0.23.1 - 2023-11-10
0.23.0 - 2023-10-26
0.22.1 - 2023-10-24
0.22.0 - 2023-10-23
0.21.0 - 2023-10-11
0.20.0 - 2023-09-21
0.19.1 - 2023-09-08
0.19.0 - 2023-08-25
0.18.1 - 2023-08-16
0.18.0 - 2023-08-14
0.17.0 - 2023-08-10
0.16.1 - 2023-08-03
0.16.0 - 2023-07-25
0.15.2 - 2023-07-19
0.15.1 - 2023-07-06
0.15.0 - 2023-07-05
0.14.1 - 2023-06-16
0.14.0 - 2023-06-14
0.13.0 - 2023-06-02
0.12.0 - 2023-05-01
0.11.0 - 2023-04-13
0.10.0 - 2023-04-07
0.9.0 - 2023-03-10
0.8.1 - 2023-03-05
0.8.0 - 2023-03-03
0.7.0 - 2023-01-10
0.6.0 - 2022-12-02
0.5.0 - 2022-11-07
0.4.0 - 2022-09-14
0.3.0 - 2022-08-30
HTTP API
0.34.1 - 2025-06-17
0.34.0 - TBD
0.33.0 - 2024-12-10
0.32.0 - 2024-11-26
0.31.0 - 2024-11-12
0.30.0 - 2024-09-24
0.29.0 - 2024-09-03
0.28.0 - 2024-06-25
0.27.0 - 2024-06-04
0.26.0 - 2024-05-14
0.25.0 - 2024-03-26
0.24.0 - 2024-03-06
0.23.0 - 2024-02-15
0.22.0 - 2024-02-06
0.21.0 - 2024-01-30
0.20.0 - 2024-01-18
0.19.0 - 2023-10-17
0.18.0 - 2023-10-11
0.17.0 - 2023-10-04
0.16.0 - 2023-09-26
0.15.0 - 2023-09-19
0.14.0 - 2023-08-29
0.13.0 - 2023-08-23
0.12.0 - 2023-08-10
0.11.0 - 2023-07-25
0.10.0 - 2023-07-06
0.9.0 - 2023-06-01
0.8.0 - 2023-05-01
0.7.0 - 2023-04-07
0.6.0 - 2023-03-10
0.5.0 - 2023-03-03
0.4.0 - 2022-12-02
0.3.0 - 2022-08-30
0.2.0 - 2021-12-10
0.1.0 - 2021-08-30
Raw API
0.6.1 - TBD
0.6.0 - 2025-05-24
0.5.6 - 2025-04-06
0.5.5 - 2024-12-01
0.5.4 - 2024-10-02
0.5.3 - 2024-10-02
0.5.1 - 2024-07-24
2024-07-20
2024-06-25
0.5.0 - 2024-05-25
0.4.6 - 2024-04-13
0.4.5 - 2024-03-25
0.4.4 - 2024-03-23
0.4.3 - 2024-02-13
0.4.2 - 2024-01-06
0.4.0 - 2023-11-08
0.3.0 - 2023-10-20
0.2.0 - 2023-07-23
0.1.0 - 2023-05-01
Rust
0.27.1 - 2025-06-17
0.27.0 - 2025-06-10
0.26.2 - 2025-06-03
0.26.1 - 2025-05-30
0.26.0 - 2025-05-28
0.25.0 - 2025-05-13
0.24.0 - 2025-04-22
0.23.0 - 2025-04-15
0.22.0 - 2025-04-01
0.21.0 - 2025-03-18
0.20.0 - 2025-02-12
0.19.0 - 2025-01-21
0.18.0 - 2025-01-08
0.17.0 - 2024-12-17
0.16.0 - 2024-11-12
0.15.0 - 2024-10-22
0.14.1 - 2024-10-08
0.14.0 - 2024-10-01
0.13.0 - 2024-09-25
0.12.1 - 2024-08-27
0.12.0 - 2024-07-30
0.11.4 - 2024-07-16
0.11.3 - 2024-07-09
0.11.2 - 2024-06-25
0.11.1 - 2024-06-11
0.11.0 - 2024-06-04
0.10.0 - 2024-05-22
0.9.1 - 2024-05-15
0.9.0 - 2024-05-14
0.8.0 - 2024-04-01
0.7.1 - 2024-03-05
0.7.0 - 2024-03-01
0.6.0 - 2024-01-16
0.5.0 - 2023-11-23
0.4.2 - 2023-10-23
0.4.1 - 2023-10-06
0.4.0 - 2023-09-21
0.3.0 - 2023-09-13
0.2.1 - 2023-08-25
0.2.0 - 2023-08-10
0.1.0 - 2023-08-02
Data
TBD (coming soon)
2025-06-17
2024-10-22
2024-05-07
2024-06-25
2024-06-18
2024-01-18
2023-11-17
2023-10-04
2023-08-29
2023-07-23
2023-05-01
2023-04-28
2023-03-07
NormalizationNormalization refers to the process of exporting data in their various
source formats to a single, unified format. Such a unified format is often
called a normalization format (or normalized schema). The primary
reason for normalizing market data is to abstract away differences between
source formats, making the data easier to work with.The normalization process is one of the most likely places where inaccuracies
or data errors are introduced. This article describes these issues, the
trade-offs for addressing them, and the reasons behind the design of
Databento's normalization schema.Examples of normalized data
For example, Nasdaq's proprietary TotalView data feed has a protocol with its own message format and provides market-by-order data,
while IEX's proprietary TOPS data feed has a completely different protocol with another message format and provides top-of-book data.
These are examples of raw data formats.
When you consume market data from a data redistributor's feed, the redistributor will have its own protocol and
message format, distinct from the venues'. This is an example of normalized data.
The most sophisticated trading firms will generally collect data directly
from their sources and normalize them to a proprietary format.
Common issues found in normalized market dataThere are many ways in which normalization can introduce data errors, lossiness
or performance issues.
Issue
Definition
Examples
Incompatible schema
The source schema and normalized schema are mismatched.
A direct market feed with an order-based schema is normalized to a vendor's schema that only provides aggregated market depth.
Truncated timestamps
A direct market feed which originally includes nanosecond-resolution timestamps is normalized to a schema that truncates the timestamps to a lower resolution.
Some vendors have a legacy data schema designed for older FIX dialects, forcing them to truncate nanosecond-resolution timestamps found in modern markets to millisecond resolution.
Discarded timestamps
A direct market feed which originally includes more than one timestamp field is normalized to a schema that discards that timestamp. This introduces imprecision when the normalized data is used for strategy backtesting.
A proprietary exchange feed may include both match (Tag 60) and sending (Tag 52) timestamps but a vendor's schema may preserve only one of the two.
Discarded or remapped sequence numbers
The normalized schema either discards the original message sequence numbers or remaps them to a vendor's own message sequence numbers.
This creates problems if you need to resolve post-trade issues with the market or your broker, as it makes it harder to identify the exact event.
Loss of price precision
The normalized schema represents prices in a type that loses precision.
Many vendors use floating point representation for prices, losing precision past 6 decimal places. This can create issues for trading Japanese yen spot rates, fixed income instruments and cryptocurrencies.
Loss of null semantics
The normalized schema represents null values in a way that changes the meaning.
Some data feeds will represent null prices with zeros or a negative value like -1. This can introduce errors downstream if the price is interpreted to be non-null. This is also problem if your application needs to handle both asset classes that can have negative prices (such as futures and spreads) and asset classes which cannot.
Loss of packet structure
The normalized schema does not preserve packet-level structure.
Many markets publish multiple events within a packet. Without packet-level structure, it may create the appearance of artificial trading opportunities between any two events within a packet.
Lossy or irreversible symbology mappings
The normalized schema adopts a proprietary symbology that is different from the original source's symbology. Sometimes, such proprietary symbology cannot be mapped back to the original.
Some vendors adopt a symbology system that only includes lead months of futures contracts, causing the far month contracts to be discarded.
Lossy abstraction
The normalized schema does not adequately standardize information across multiple datasets, resulting in the end user needing to understand the specifications of the various source schemas anyway in order to determine the lost information.
This often happens when normalizing less commonly used features such as matching engine statuses or instrument definitions. This puts significant burden on the user to study the specifications of various data feeds to understand the lost information.
Statelessness
The normalized schema provides incremental changes but does not provide snapshots or replay of order book state.
This presents an issue when using the normalized data in real-time, as the user loses information in the event of a disconnection or late join.
Coalescing
The normalized schema aggregates the information at a lower granularity.
A vendor may coalesce a feed of tick data with second bar aggregates or subsample a source feed.
Conflation
A normalized feed batches multiple updates into one at some lower frequency, to alleviate bandwidth limitations. Often present along with coalescing.
This is a common practice for retail brokerages, whose data feeds are designed more for display use and consumption over sparse WAN links.
Dropped packets
A normalized data feed deliberately discards data when the network or system is unable to keep up.
This is often present when the source feed or upstream parts of the vendor's infrastructure uses UDP for transmission.
Buffering
The data server sends stale data either because there is insufficient network bandwidth or the client is reading too slowly. The client misinterprets the stale data, either obscuring this effect or injecting incorrect timestamps.
This often manifests when the data feed uses TCP for transmission - which is a common practice when disseminating data over WAN links.
Ex post cleaning
A data source is cleaned, during the normalization process, using future information. This enhances the historical data with artificial information that may not have been actionable in real-time.
The data may be reordered; trades that were canceled after the end of market session may be removed, or prices may be adjusted with information from a future rollover or dividend event.
Schema bloat
A normalized schema represents some data fields with types that take up unnecessary space or make the data more difficult to compress, which increases storage costs and reduces application performance.
Common cases of this include representing timestamps as ISO 8601 strings or prices as strings, especially on vendor feeds that use JSON encoding.
Our normalization schema is designed to mitigate most of these issues.Why use normalized data?Though it may seem counterintuitive, some degree of lossiness introduced
during normalization can be preferable.A normalized schema that has too many
data fields, as a result of trying to preserve information from too many
different source schemas, is hard to use.Here are some ways in which lossiness can be useful:
Discarding unnecessary data fields can reduce storage and bandwidth
requirements, and improve application performance. For example, many
strategies execute at time scales where extra timestamps are
unnecessary.
Most status or reference data events are irrelevant for any given
business use case. For example, many users only trade during the
regular market session and their applications do not need to be
aware of special matching conditions that are more typically found
outside of regular hours or during pre-market.
Floating point prices can be easier to use and the modeling error
introduced by them could be negligible compared to other, more
likely sources of error for the given use case.
Order book snapshots can be unnecessary on liquid products whose
orders turnover very quickly, as pre-existing orders in the snapshot
will eventually be filled or canceled - a process which is commonly
referred to as natural refresh.
It should be noted that normalized data is not necessarily going
to be smaller or have a simpler specification than the source data.
If your use case only requires a single dataset, there may be complexity using
normalized data whose schema was designed to accommodate differences between
multiple datasets.Moreover, normalized data will often be smaller or have a smaller
specification.We normalize to our proprietary Databento Binary Encoding DBN. Python Python C++ Rust HTTP/Raw
